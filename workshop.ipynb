{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Union.ai Serverless Workshop\n",
    "\n",
    "Welcome to the Union.ai Serverless Workshop! In this workshop, we will cover:\n",
    "\n",
    "1. Setup workspace and connect to Union.ai Serverless.\n",
    "2. Creating a GPT3.5-based retrieval augmented generation (RAG) workflow.\n",
    "3. Deploying a Streamlit app to interact with the Union workflow.\n",
    "4. Transitioning to an open weights LLM-based RAG workflow with Ollama.\n",
    "\n",
    "## Setup Workspace\n",
    "\n",
    "1. If you are not signed into Google, sign in by clicking the \"Sign in\" on the upper right of this page.\n",
    "2. If you have not already, sign up for Union Serverless at: https://signup.union.ai/?group=workshop\n",
    "3. Navigate to https://serverless.union.ai\n",
    "\n",
    "**Note:** running the cell below will cause the Colab session to restart due\n",
    "to a reinstalled dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/unionai-oss/union-rag.git\n",
    "    %cd union-rag\n",
    "    %pip install -r _workshop/requirements.lock.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate on Colab\n",
    "\n",
    "To authenticate with Severless run, and go to the link provided to authenticate your CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/union-rag\n",
    "!unionai create login device-flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure everything is working, run this sample workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unionai run --remote _workshop/starter.py main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the link provided to see your execution on Union!\n",
    "\n",
    "## GPT3.5-based RAG workflow\n",
    "\n",
    "In this first part, we're going to run a RAG workflow defined as a Flyte workflow,\n",
    "using LangChain as the LLM pipelining tool and GPT3.5 as the underlying LLM.\n",
    "\n",
    "### Create secrets\n",
    "\n",
    "Let's create a Union secret for an OpenAI API key. The workshop runner will now\n",
    "provide a limited key for the purposes of this workshop.\n",
    "\n",
    "> **Note**: Don't get too excited üôÇ, this key can only use `gpt-3.5-turbo` and\n",
    "> has a limit of $50. We will delete this key after the workshop.\n",
    "\n",
    "Once you have the key, create a secret on Union with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unionai create secret openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see all of your secrets, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unionai get secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running into an issue with secrets, uncomment the following code to\n",
    "delete the secret and try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unionai delete secret openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the knowledge base\n",
    "\n",
    "The first stage in building a RAG application is to create knowledge base that\n",
    "we represent as a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unionai run --copy-all --remote union_rag/langchain.py create_knowledge_base \\\n",
    "    --exclude_patterns '[\"/api/\", \"/_tags/\"]' \\\n",
    "    --limit 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step will that about 5-10 minutes, so in the mean time, let's review a few\n",
    "things about this part of the workflow.\n",
    "\n",
    "This workflow highlights a few important Flyte features:\n",
    "- `ImageSpec`: abstracting the Docker container\n",
    "- Tasks: the core building block of Flyte\n",
    "- Workflows: composing building blocks together\n",
    "- Caching and cache versions\n",
    "- Resource requests\n",
    "\n",
    "And it also highlights Union features:\n",
    "- Artifacts and partitioning\n",
    "- Remote image-building service\n",
    "\n",
    "Let's also explore the UI to get some insights into what's going on in this workflow:\n",
    "- List view\n",
    "- Graph view\n",
    "- Timeline view\n",
    "- View utilization\n",
    "- Live logs\n",
    "\n",
    "Once the `create_knowledge_base` workflow is complete, we can take a look at the\n",
    "artifact that was created through the UI.\n",
    "\n",
    "### Ask a Flyte-related question\n",
    "\n",
    "We can then use the `VectorStore` knowledge base we created to ask a flyte-related\n",
    "question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unionai run --copy-all --remote union_rag/langchain.py ask --question 'what is flytekit?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to note in this workflow:\n",
    "- We're using the `openai_api_key` secret that we made earlier to use GPT3.5.\n",
    "- We rehydrate the `FAISS` vector store to use for RAG.\n",
    "- We create question-answering pipeline with `load_qa_with_sources_chain` using LangChain\n",
    "\n",
    "We also use `Artifact`s and partitions to consume the correct vector store:\n",
    "\n",
    "- We use the `partition_keys` argument to define the artifact partitions\n",
    "- We define the partitioned artifact as a task output with `Artifact(partition=Inputs.arg)`\n",
    "- We can consume artifact partitions in downstream workflow with the `Artifact.query(partition=\"<value>\")`\n",
    "\n",
    "Evaluation is also a big part of the RAG lifecycle. To do this, it's helpful to\n",
    "collect feedback. We can use gate nodes to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unionai run --copy-all --remote union_rag/langchain.py ask_with_feedback --question 'what is flytekit?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the execution on the UI and you'll see the `ask_with_feedback` workflow\n",
    "has a step in the end where you can give it some feedback. In this case, it's\n",
    "an unstructured text field, so you can say something like \"thumbs-up\" or \"thumbs-down\".\n",
    "\n",
    "In the next step of this workshop, we'll see how you can interact with this\n",
    "RAG workflow through a Streamlit app.\n",
    "\n",
    "## Build a Streamlit App\n",
    "\n",
    "First, create a Union API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unionai create app streamlit-rag-app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the API key somewhere secure!\n",
    "\n",
    "Then, follow these steps to deploy the streamlit app that you can use to interact\n",
    "with the RAG workflow:\n",
    "\n",
    "- Sign up for an account on streamlit cloud: https://share.streamlit.io/signup\n",
    "- Fork the `union-rag` repo: https://github.com/unionai-oss/union-rag\n",
    "- Click on **Create App** > **Yup, I have an app**, then add the information\n",
    "  needed to deploy:\n",
    "  - **Repository**: `<github_username>/union-rag`\n",
    "  - **Branch**: `main`\n",
    "  - **Main file path**: streamlit/app.py\n",
    "- Under **Advanced settings**, provide the Union secret:\n",
    "  ```\n",
    "  UNIONAI_SERVERLESS_API_KEY = \"<MY_SECRET>\"\n",
    "  ```\n",
    "- Then click **Deploy!**\n",
    "\n",
    "After a few seconds, you should be redirected to the streamlit app as it builds.\n",
    "You should see a chat input box in the bottom of the screen.\n",
    "\n",
    "Try typing in a flyte-related question! Note: responses will take about 20-30\n",
    "seconds.\n",
    "\n",
    "Let's go through how this application works:\n",
    "- It uses `UnionRemote` to connect to Union Serverless.\n",
    "- When a user types in a question in the chat box, this kicks off an with `UnionRemote.execute`.\n",
    "- When the response comes back, we keep track of the execution ids associated\n",
    "  with each response.\n",
    "- When a users clicks on one of the üëç or üëé buttons, we use `UnionRemote.set_signal`\n",
    "  to programmatically provide feedback on the response.\n",
    "\n",
    "\n",
    "## Ollama-based RAG workflow\n",
    "\n",
    "First, we need to re-create the knowledge base to use an open-weights embedder.\n",
    "In this case, we'll use the `all-MiniLM-L6-v2` sentence transformer available\n",
    "through Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unionai run --copy-all --remote union_rag/langchain.py create_knowledge_base \\\n",
    "    --exclude_patterns '[\"/api/\", \"/_tags/\"]' \\\n",
    "    --embedding_type \"huggingface\" \\\n",
    "    --limit 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we wait for this new knowledge base to build, we can let's dive into a few\n",
    "aspects of this workflow that are worth noting.\n",
    "\n",
    "Since we're using Union to host an Ollama server, we need to provision a GPU\n",
    "to use it effectively. We do this by:\n",
    "\n",
    "- Specifying the `requests=Resources(cpu=\"4\", mem=\"24Gi\", gpu=\"1\")` argument.\n",
    "- Defining the `accelerator=accelerators.T4` argument.\n",
    "\n",
    "It's also important to know how we're setting up the `Ollama` dependencies and\n",
    "server using `ImageSpec` and task decorators:\n",
    "\n",
    "- We use the `with_apt_packages` to install additional dependencies that we need\n",
    "  to use Ollama.\n",
    "- We use the `with_commands` to invoke additional commands to install Ollama and\n",
    "  preload the llama3 model into the container.\n",
    "- The `ollama_server` task function decorator starts an Ollama server to run\n",
    "  the RAG pipeline and tears it down when it's done.\n",
    "\n",
    "Next, we invoke the `ask_ollama` workflow to run a RAG workflow that uses `phi3`\n",
    "to answer our question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unionai run --copy-all --remote union_rag/langchain.py ask_ollama --question 'what is flytekit?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does this take a lot longer than the GPT3.5 RAG workflow?\n",
    "\n",
    "- We're spinning up an ephemeral T4 GPU, starting an Ollama server, then running our RAG task.\n",
    "- With a regular `@task` this resource is spun down after the task is complete\n",
    "- The GPT3.5 workflow simply does an API call on a Union CPU resource.\n",
    "- We're working on ways to initialize long-running resources to we don't incur\n",
    "  additional overhead due to ephemeral resources (stay tuned!).\n",
    "\n",
    "### Customizing the prompts\n",
    "\n",
    "As you can see, the smaller LLMs are less useful than the off-the-shelf SOTA\n",
    "models like GPT3.5 that you can access via an API.\n",
    "\n",
    "The iterative part of the RAG lifecycle is to do prompt engineering to customize\n",
    "how the RAG system interprets the retrieved information to help it answer your\n",
    "question.\n",
    "\n",
    "This involves creating a [RAG Evaluation pipeline](https://huggingface.co/learn/cookbook/en/rag_evaluation),\n",
    "which we won't have time to create in todays workshop, but to end, let's go\n",
    "through how we can customize the prompts that are using in the langchain pipeline\n",
    "that we're using to implement our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unionai run --copy-all --remote union_rag/langchain.py ask_ollama --question 'what is flytekit?' --prompt_template \"$(cat PROMPT_TEMPLATE.txt)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this workshop, you learned how to:\n",
    "- Set up a development environment on Union Serverless\n",
    "- Run a RAG workflow using Langchain and GPT3.5\n",
    "- Deploy a Streamlit app that interacts with Union Serverless\n",
    "- Swap out GPT3.5 for open weights models like `all-MiniLM-L6-v2` for sentence\n",
    "  embeddings and `phi3` as the response model.\n",
    "- Customizing prompts via `PromptTemplate`s in Langchain.\n",
    "\n",
    "Thank you for attending!\n",
    "\n",
    "\n",
    "## Bonus: Create a Slackbot\n",
    "\n",
    "If you're interested in high-latency chat use cases, check out the slack app\n",
    "deployment section in the repo [README](https://github.com/unionai-oss/union-rag?tab=readme-ov-file#slack-app-deployment)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
